data:
  train_ratio: 0.85
  test_val_ratio: 0.35
  data_dir: /root/dataset/dataset
  dataset_name: "Ita-PVS"     # "Ita-PVS" | "Neurovoz" | "Addresso"
  target_sr: 16000
  mono: true
  normalize: "peak"           # "peak" | "rms"
  fixed_duration_s: 5.0
  resample: true
  pad_mode: "silence"         # zeri
  label_map: { healthy: 0, parkinson: 1 }

model:
  dropout: 0.35                             # Aumentato per dataset piccolo (prevenire overfitting)
  branch: "classical_svm"     # "classical_svm" | "classical_mlp" | "cnn" | "transformers_mlp"
  cnn:
    in_type: "waveform"                      # "waveform" | "spectrogram" - waveform per semplicità
  mlp:
    hidden_layers: [384, 192]               # Ridotto per dataset piccolo (800 campioni)
    dropout: 0.4                            # Aumentato per prevenire overfittingprevenire overfitting
  svm:
    kernel: "rbf"                          # RBF kernel per non-linearità
    C: 0.1                                 # Ridotto per più regolarizzazione (dataset piccolo)
    gamma: "auto"                          # Auto per evitare overfitting

features:
  classical:
    n_mfcc: 13
    mfcc_delta: true
    energy: true
    pitch: false
    jitter_shimmer: true
  spectrogram:
    n_mels: 32                             # Ridotto per dataset piccolo (meno complessità)
    n_fft: 256                             # Ridotto per evitare overfitting
    hop_length: 128                        # Valore standard ottimizzato
    log: true
  transformers:
    model_name: "facebook/wav2vec2-base-960h"     # Miglior modello dai test (83.05% accuracy)
    pooling: "mean"                             # "cls" | "mean"
    freeze_backbone: false                      # Unfreeze per fine-tuning completo
    max_length: 48000                          # Ridotto per dataset piccolo (3s * 16kHz)
    layer: -1                                  # Usa ultimo layer per rappresentazioni migliori
                                                # Modelli disponibili:
                                                # - facebook/wav2vec2-base-960h
                                                # - ALM/wav2vec2-base-audioset
                                                # - facebook/hubert-base-ls960
                                                # - ALM/hubert-base-audioset
                                                # - microsoft/wavlm-base-plus
                                                # - microsoft/wavlm-base

training:
  epochs: 35                               # Aumentato per dataset piccolo (più iterazioni necessarie)
  batch_size: 32                           # Ridotto per dataset piccolo (800/32 = 25 batch)
  optimizer: adamw                         # AdamW per migliore regolarizzazione
  max_lr: 0.0005                          # LR ridotto per convergenza più stabile
  min_lr: 0.000005                        # Min LR proporzionale
  warmup_ratio: 0.25                      # Warmup aumentato per dataset piccolo
  weight_decay: 0.015                     # Regolarizzazione L2 più forte per overfitting
  checkpoint_dir: checkpoints/
  model_name: best_model
  device: "cuda"              # "cuda" | "mps" | "cpu"
  evaluation_metric: accuracy
  best_metric_lower_is_better: false
  seed: 42  
  # Validation Split
  validation_split: 0.15                     # Ridotto per massimizzare training data (800*0.85=680 train)

plot: [accuracy, loss]