data:
  train_ratio: 0.85
  test_val_ratio: 0.35
  data_dir: /root/dataset/dataset
  dataset_name: "Ita-PVS"     # "Ita-PVS" | "Neurovoz" | "Addresso"
  target_sr: 16000
  mono: true
  normalize: "peak"           # "peak" | "rms"
  fixed_duration_s: 5.0
  resample: true
  pad_mode: "silence"         # zeri
  label_map: { healthy: 0, parkinson: 1 }

model:
  dropout: 0.35                             # Aumentato per dataset piccolo (prevenire overfitting)
  branch: "transformers_mlp"     # "classical_svm" | "classical_mlp" | "cnn" | "transformers_mlp"
  cnn:
    in_type: "waveform"                      # "waveform" | "spectrogram" - waveform per semplicità
  mlp:
    hidden_layers: [256, 128]              # Architettura più semplice per evitare overfitting
    dropout: 0.3                           # Dropout ottimizzato per bilanciamento
  svm:
    kernel: "linear"                       # Linear per dataset piccolo (meno overfitting)
    C: 10.0                                # Aumentato per più flessibilità
    gamma: "scale"                         # Scale per bilanciamento migliore
    class_weight: "balanced"               # CRITICO: bilancia classi sbilanciate

features:
  classical:
    n_mfcc: 20                             # Aumentato per più informazioni
    mfcc_delta: true                       # Delta features per dinamica temporale
    mfcc_delta2: true                      # Delta-delta per accelerazione
    energy: true                           # Energia RMS
    pitch: true                            # Pitch fondamentale (importante per voce)
    pitch_method: "pyin"                   # Metodo PYIN per pitch robusto
    pitch_fmin: 50                         # Frequenza minima pitch (voce umana)
    pitch_fmax: 400                        # Frequenza massima pitch
    jitter_shimmer: true                   # Variabilità pitch e ampiezza
    spectral_features: true                # Features spettrali aggiuntive
    zero_crossing_rate: true               # Tasso attraversamento zero
  spectrogram:
    n_mels: 32                             # Ridotto per dataset piccolo (meno complessità)
    n_fft: 256                             # Ridotto per evitare overfitting
    hop_length: 128                        # Valore standard ottimizzato
    log: true
  transformers:
    model_name: "facebook/wav2vec2-base-960h"     # Miglior modello dai test (83.05% accuracy)
    pooling: "mean"                             # "cls" | "mean"
    freeze_backbone: false                      # Unfreeze per fine-tuning completo
    max_length: 48000                          # Ridotto per dataset piccolo (3s * 16kHz)
    layer: -1                                  # Usa ultimo layer per rappresentazioni migliori
                                                # Modelli disponibili:
                                                # - facebook/wav2vec2-base-960h
                                                # - ALM/wav2vec2-base-audioset
                                                # - facebook/hubert-base-ls960
                                                # - ALM/hubert-base-audioset
                                                # - microsoft/wavlm-base-plus
                                                # - microsoft/wavlm-base

training:
  epochs: 25                               # Aumentato ulteriormente per convergenza graduale
  batch_size: 24                           # Ridotto per dataset piccolo (migliore gradiente)
  optimizer: adamw                         # AdamW per migliore regolarizzazione
  max_lr: 0.0006                          # LR ridotto per convergenza più stabile
  min_lr: 0.000006                        # Min LR proporzionale
  warmup_ratio: 0.35                      # Warmup aumentato per convergenza graduale
  weight_decay: 0.02                      # Regolarizzazione aumentata
  checkpoint_dir: checkpoints/
  model_name: best_model
  device: "cuda"              # "cuda" | "mps" | "cpu"
  evaluation_metric: accuracy
  best_metric_lower_is_better: false
  seed: 42  
  # Validation Split
  validation_split: 0.20                     # Ridotto per massimizzare training data (800*0.85=680 train)

plot: [accuracy, loss]